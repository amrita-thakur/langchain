{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Approach 1: Saving the FAISS Vector Store as a File**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load and Split Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_documents(directory_path: str = \"data\", chunk_size: int = 500, chunk_overlap: int = 0):\n",
    "    \"\"\"\n",
    "    Load PDF documents from a directory and split them into chunks.\n",
    "\n",
    "    Args:\n",
    "        chunk_size (int): Size of each chunk.\n",
    "        chunk_overlap (int): Overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "        list: List of document chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.isdir(directory_path):\n",
    "        raise ValueError(f\"Directory path not found: {directory_path}\")\n",
    "    \n",
    "    pdf_loader = DirectoryLoader(directory_path, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "    pdf_documents = pdf_loader.load()\n",
    "\n",
    "    chunk_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, \n",
    "                                                    chunk_overlap=chunk_overlap,\n",
    "                                                    add_start_index=True,)\n",
    "    document_chunks = chunk_splitter.split_documents(pdf_documents)\n",
    "\n",
    "    return document_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Save to FAISS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_faiss(document_chunks: list, vectorstore_filename: str = \"faiss_vectorstore.pkl\"):\n",
    "    \"\"\"\n",
    "    Initialize the language model and vector store.\n",
    "\n",
    "    Args:\n",
    "        document_chunks (list): List of document chunks.\n",
    "        db_path (str): path to store data in FAISS database locally.\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={'device': \"cpu\"})\n",
    "    vectorstore_contents = FAISS.from_documents(document_chunks, embeddings)\n",
    "\n",
    "    with open(vectorstore_filename, \"wb\") as f:\n",
    "        pickle.dump(vectorstore_contents, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load FAISS vector store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faiss_vector_store(vectorstore_filename: str = \"faiss_vectorstore.pkl\"):\n",
    "    \"\"\"\n",
    "    Load FAISS vector store.\n",
    "\n",
    "    Args:\n",
    "        db_directory_path (str): path to FAISS database.\n",
    "    Returns:\n",
    "        FAISS vector store\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        with open(vectorstore_filename, \"rb\") as f:\n",
    "            vector_store = pickle.load(f)\n",
    "\n",
    "        retriever=vector_store.as_retriever()\n",
    "\n",
    "        return retriever\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = load_faiss_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001C2FE305300>, search_kwargs={})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Approach 2: Saving the FAISS Vector Store as a Directory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Save to FAISS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_faiss(document_chunks: list, db_path: str = \"faiss_db\"):\n",
    "    \"\"\"\n",
    "    Initialize the language model and vector store.\n",
    "\n",
    "    Args:\n",
    "        document_chunks (list): List of document chunks.\n",
    "        db_path (str): path to store data in FAISS database locally.\n",
    "    \"\"\"\n",
    "    # Clear out the existing database directory if it exists\n",
    "    if os.path.exists(db_path):\n",
    "        shutil.rmtre\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\", model_kwargs={'device': \"cpu\"})\n",
    "    faiss_vector_database = FAISS.from_documents(document_chunks, embeddings)\n",
    "\n",
    "    faiss_vector_database.save_local(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = load_and_split_documents()\n",
    "save_to_faiss(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load FAISS Vector Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faiss_vector_store(db_directory_path: str = \"faiss_db\"):\n",
    "    \"\"\"\n",
    "    Load FAISS vector store.\n",
    "\n",
    "    Args:\n",
    "        db_directory_path (str): path to FAISS database.\n",
    "    Returns:\n",
    "        FAISS vector store\n",
    "    \"\"\"\n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\", model_kwargs={'device': \"cpu\"})\n",
    "        vector_store = FAISS.load_local(db_directory_path, embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "        retriever=vector_store.as_retriever()\n",
    "\n",
    "        return retriever\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001C2CAC96020>, search_kwargs={})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = load_faiss_vector_store()\n",
    "retriever"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
